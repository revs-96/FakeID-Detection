# -*- coding: utf-8 -*-
"""LR_GD_code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_BhITGfrwreJmu-OyxWIe1c8KyqhNtmC
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import math

df = pd.read_csv(r'/content/test.csv')
df.head(10)

df["fake"].unique()

dfs = [x for _, x in df.groupby('fake')]
df0 = dfs[1].reset_index(drop=True)
df1 = dfs[0].drop(columns=["fake"]).reset_index(drop=True)

df0.head(10)

df0["fake"] = 0
df1["fake"] = 1

train_df0 = df0.iloc[0: int(len(df0)*0.8)]
train_df1 = df1.iloc[0: int(len(df1)*0.8)]

X_train_data = pd.concat([train_df0.drop(columns=["fake"]), train_df1.drop(columns=["fake"])]).reset_index(drop=True)
X_train_data

y_train = pd.concat([train_df0["fake"], train_df1["fake"]]).to_numpy()
y_train

test_df0 = df0.iloc[int(len(df0)*0.8):]
test_df1 = df1.iloc[int(len(df1)*0.8):]

X_test_data = pd.concat([test_df0.drop(columns=["fake"]), test_df1.drop(columns=["fake"])]).reset_index(drop=True)
y_test = pd.concat([test_df0["fake"], test_df1["fake"]]).to_numpy()

mean_train = X_train_data.mean()
std_train = X_train_data.std()

mean_train

std_train

X_train_standard = (X_train_data - mean_train)/ std_train
X_train_standard.head()

X_train_standard = X_train_standard.to_numpy()
X_train_standard.shape

X_train = np.hstack([np.ones((X_train_standard.shape[0],1)), X_train_standard]) # add bias term
X_train.shape

X_test_standard = (X_test_data - mean_train)/ std_train
X_test_standard.head()

X_test_standard = X_test_standard.to_numpy()
X_test_standard.shape

X_test = np.hstack([np.ones((X_test_standard.shape[0],1)), X_test_standard]) # add bias term
X_test.shape

#Prepare the Logistic Regression algorithm

def cost(y_truth, y_proba):
    loss = - y_truth*np.log(y_proba) - (1- y_truth)*np.log(1 - y_proba)
    return loss

def gradient_compute(X, y, y_proba):
    '''
    X: Input Feature Array of NxD shape
    y: ground truth output of N shape
    y_proba: probability of N shape
    '''
    assert y.ndim == 1, "y should be one dimensional"
    return np.dot((y_proba - y), X)

def probability(w, X):
    y_proba = np.array([])
    for i in range(len(X)):
        proba = sigmoid(np.dot(w, X[i]))
        y_proba = np.append(y_proba, proba)
    return y_proba

def prediction(y_proba):
    y_pred = np.array([], dtype=np.int16)

    for i in range(len(y_proba)):
        if y_proba[i] > 0.5:
            pred = 1
        else:
            pred = 0
        y_pred =np.append(y_pred, pred)
    return y_pred

def fit(X, y, epochs=10, learning_rate=0.01):
    N = X.shape[0]                     # number of training samples
    w = np.ones((1, X_train.shape[1])) # initialize the weights
    cost_list = []
    old_cost = 100
    for k in range(epochs):
        y_proba = probability(w, X)
        w = w - learning_rate* gradient_compute(X, y, y_proba) # update the weights after one epoch

        loss = 0
        total_cost = 0
        for i in range(N):
            loss += cost(y[i], y_proba[i])
        total_cost = loss / N
        cost_list.append(total_cost)
    return w, cost_list

def fit(X, y, epochs=100, learning_rate=0.01):
    # initialize weights
    w = np.zeros(X.shape[1])
    cost_list = []
    m = len(y)

    def sigmoid(z):
        """Compute sigmoid function"""
        return 1 / (1 + np.exp(-z))

    # gradient descent
    for epoch in range(epochs):
        y_pred = sigmoid(np.dot(X, w))
        error = y_pred - y
        grad = np.dot(X.T, error) / m
        w -= learning_rate * grad
        cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
        cost_list.append(cost)

    return w, cost_list

epochs = 30
learning_rate = 0.01
w_offline, cost_list_offline = fit(X_train, y_train, epochs = epochs, learning_rate = learning_rate)
print(f"Weights: {w_offline}")
plt.plot(cost_list_offline)
plt.xlabel("Iterations")
plt.ylabel("Loss")

def create_mini_batches(X, y, batch_size):
    mini_batches = []
    data = np.hstack((X, y))
    np.random.shuffle(data)
    n_minibatches = data.shape[0] // batch_size
    i = 0
 
    for i in range(n_minibatches):
        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    if data.shape[0] % batch_size != 0:
        mini_batch = data[i * batch_size:data.shape[0]]
        X_mini = mini_batch[:, :-1]
        Y_mini = mini_batch[:, -1].reshape((-1, 1))
        mini_batches.append((X_mini, Y_mini))
    return mini_batches

def fit_mini(X, y, batch_size = 10, epochs=10, learning_rate=0.01):
    N = X.shape[0]
    w = np.ones((1, X_train.shape[1]))
    cost_list = []
    number_of_batch = N // batch_size 
    old_cost = 100
    for k in range(epochs):
        mini_batches = create_mini_batches(X, y.reshape(len(y),1), batch_size)
        for mini_batch in mini_batches:
            X_mini, y_mini = mini_batch
            y_proba = probability(w, X_mini)
            w = w - learning_rate* gradient_compute(X_mini, np.squeeze(y_mini), y_proba)

            loss = 0
            total_cost = 0
            for i in range(len(y_mini)):
                loss += cost(y_mini[i], y_proba[i])
            total_cost = loss / len(y_mini)
            cost_list.append(total_cost)
    return w, cost_list

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

epochs = 30
learning_rate = 0.01
w_mini, cost_list_mini = fit_mini(X_train, y_train, epochs=epochs, learning_rate=learning_rate)
print(f"Weights: {w_mini}")
plt.plot(cost_list_mini)
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.show()

def compute_confusion_matrix(true, pred):
    '''Computes a confusion matrix using numpy.'''

    K = len(np.unique(true)) # Number of classes 
    result = np.zeros((K, K))

    for i in range(len(true)):
        result[true[i]][pred[i]] += 1

    return result

def accuracy(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    return (tp+tn)/(tp+tn+fp+fn)

def sensitivity(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    return tp/(tp+fn)

def specificity(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    return tn/(tn+fp)

def precision(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    return tp/(tp+fp)

def f1_score(conf_matrix):
    tn, fp, fn, tp = conf_matrix.ravel()
    precision = tp/(tp+fp)
    recall = tp/(tp+fn)
    f1_score = 2 * (precision * recall)/(precision + recall)
    return f1_score

def log_loss(ground_truth, y_proba):
    log_loss = -((ground_truth * np.log(y_proba)) + ((1-ground_truth) * np.log(1-y_proba))).mean()
    return log_loss

y_train_proba = probability(w_offline, X_train)
y_train_pred = prediction(y_train_proba)

conf_matrix_train = compute_confusion_matrix(y_train, y_train_pred)
conf_matrix_train

accuracy(conf_matrix_train)

